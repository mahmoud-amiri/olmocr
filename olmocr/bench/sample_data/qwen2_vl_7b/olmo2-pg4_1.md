The mix used for this stage is shown in Table 1. It consists of approximately 3.9 trillion tokens, with over 95% derived from web data. We refer to this set as OLMo Mix 1124. This is the same pretraining data used in OLMoE. We combine data from DCLM and Dolma. From DCLM, we use the "baseline 1.0" mix. From Dolma, we use the arXiv, OpenWebMath, and Algebraic Stack. arXiv, OpenWebMath, and Algebraic Stack were originally part of ProofPile II. Finally, we include code from StarCoder, which is derived from permissively-licensed repositories from GitHub. In an attempt to include higher quality code, we remove any document from a repository with fewer than 2 stars on GitHub. Further, through manual inspection of this source, we found it to contain documents encoded in binary format or containing mostly numerical content; to remove them, we discarded documents whose most frequent word constitutes over 30% of the document, or whose top-2 most frequent words constitute over 50% of the document. To mitigate possible training loss spikes, we remove documents with repeated sequences of 32 or more n-grams. We report details and show effectiveness of this intervention in Section ยง3.1.

After the initial pretraining stage on mostly web data, we further train with a mixture of web data that has been more restrictively filtered for quality and a collection of domain-specific high quality data, much of which is synthetic. The purpose of this mixture is to imbue the model with math-centric skills and provide focused exposure to STEM references and high quality text. We generate several variants of this mixture, with varying sizes, but generally refer to this mixture as Dolmino Mix 1124. The base sources from which Dolmino Mix 1124 is subsampled are described in Table 2. We refer the reader to Section ยง4 for a deep dive detailing our processes for subsampling for experimenting and curating data for this mix.