The mix used for this stage is shown in Table 1. It consists of approximately 3.9 trillion tokens, with over 95% derived from web data. We refer to this set as OLMo Mix 1124. This is the same pretraining data used in OLMoE. We combine data from DCLM and Dolma. From DCLM, we use the "baseline 1.0" mix. From Dolma, we use the arXiv, OpenWebMath, and Algebraic Stack. Finally, we include code from StarCoder, which is derived from permissively-licensed repositories from GitHub.